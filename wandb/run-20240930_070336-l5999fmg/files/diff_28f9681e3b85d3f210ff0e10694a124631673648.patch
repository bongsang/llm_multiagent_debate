diff --git a/math/gen_math.py b/math/gen_math.py
index 687f116..c3937da 100644
--- a/math/gen_math.py
+++ b/math/gen_math.py
@@ -1,9 +1,17 @@
-import openai
+from openai import OpenAI
 import json
 import numpy as np
 import time
 import pickle
 from tqdm import tqdm
+import weave  # Import Weave
+import wandb
+
+client = OpenAI()
+OPENAI_MODEL = "gpt-4o-mini"
+
+# Initialize the Weave project
+weave.init('llm-multiagent_debate_math')
 
 def parse_bullets(sentence):
     bullets_preprocess = sentence.split("\n")
@@ -22,13 +30,13 @@ def parse_bullets(sentence):
 
     return bullets
 
-
+# Weave will track the inputs, outputs, and code of this function
+@weave.op()
 def generate_answer(answer_context):
     try:
-        completion = openai.ChatCompletion.create(
-                  model="gpt-3.5-turbo-0301",
-                  messages=answer_context,
-                  n=1)
+        completion = client.chat.completions.create(
+            model=OPENAI_MODEL, messages=answer_context, n=1
+        )
     except:
         print("retrying due to an error......")
         time.sleep(20)
@@ -36,27 +44,34 @@ def generate_answer(answer_context):
 
     return completion
 
-
+# Weave will track this function as well
+@weave.op()
 def construct_message(agents, question, idx):
 
     # Use introspection in the case in which there are no other agents.
     if len(agents) == 0:
-        return {"role": "user", "content": "Can you verify that your answer is correct. Please reiterate your answer, making sure to state your answer at the end of the response."}
+        return {
+            "role": "user",
+            "content": "Can you verify that your answer is correct? Please reiterate your answer, making sure to state your answer at the end of the response.",
+        }
 
     prefix_string = "These are the recent/updated opinions from other agents: "
 
     for agent in agents:
         agent_response = agent[idx]["content"]
-        response = "\n\n One agent response: ```{}```".format(agent_response)
+        response = f"\n\n One agent response: ```{agent_response}```"
 
-        prefix_string = prefix_string + response
+        prefix_string += response
 
-    prefix_string = prefix_string + "\n\n Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response.".format(question)
+    prefix_string += (
+        "\n\n Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response."
+    )
     return {"role": "user", "content": prefix_string}
 
-
+# Weave will track this function as well
+@weave.op()
 def construct_assistant_message(completion):
-    content = completion["choices"][0]["message"]["content"]
+    content = completion.choices[0].message.content
     return {"role": "assistant", "content": content}
 
 def parse_answer(sentence):
@@ -69,7 +84,6 @@ def parse_answer(sentence):
         except:
             continue
 
-
 def most_frequent(List):
     counter = 0
     num = List[0]
@@ -82,34 +96,56 @@ def most_frequent(List):
 
     return num
 
-
 if __name__ == "__main__":
-    answer = parse_answer("My answer is the same as the other agents and AI language model: the result of 12+28*19+6 is 550.")
+    
+    answer = parse_answer(
+        "My answer is the same as the other agents and AI language model: the result of 12+28*19+6 is 550."
+    )
 
     agents = 2
     rounds = 3
     np.random.seed(0)
 
-    evaluation_round = 100
+    # evaluation_round = 100
+    evaluation_round = 1
     scores = []
 
+    # Initialize WandB
+    wandb.init(
+        project="llm-multiagent_debate_math",
+        config={
+            "agents": agents,
+            "rounds": rounds,
+            "evaluation_rounds": evaluation_round,
+        })
+
     generated_description = {}
 
-    for round in tqdm(range(evaluation_round)):
+    for eval_round in tqdm(range(evaluation_round)):
         a, b, c, d, e, f = np.random.randint(0, 30, size=6)
 
         answer = a + b * c + d - e * f
-        agent_contexts = [[{"role": "user", "content": """What is the result of {}+{}*{}+{}-{}*{}? Make sure to state your answer at the end of the response.""".format(a, b, c, d, e, f)}] for agent in range(agents)]
-
-        content = agent_contexts[0][0]['content']
-        question_prompt = "We seek to find the result of {}+{}*{}+{}-{}*{}?".format(a, b, c, d, e, f)
-
-        for round in range(rounds):
+        agent_contexts = [
+            [
+                {
+                    "role": "user",
+                    "content": f"What is the result of {a}+{b}*{c}+{d}-{e}*{f}? Make sure to state your answer at the end of the response.",
+                }
+            ]
+            for _ in range(agents)
+        ]
+
+        content = agent_contexts[0][0]["content"]
+        question_prompt = f"We seek to find the result of {a}+{b}*{c}+{d}-{e}*{f}?"
+
+        for round_num in range(rounds):
             for i, agent_context in enumerate(agent_contexts):
 
-                if round != 0:
-                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]
-                    message = construct_message(agent_contexts_other, question_prompt, 2*round - 1)
+                if round_num != 0:
+                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i + 1 :]
+                    message = construct_message(
+                        agent_contexts_other, question_prompt, 2 * round_num - 1
+                    )
                     agent_context.append(message)
 
                     print("message: ", message)
@@ -123,30 +159,32 @@ if __name__ == "__main__":
         text_answers = []
 
         for agent_context in agent_contexts:
-            text_answer = string =  agent_context[-1]['content']
+            text_answer = agent_context[-1]["content"]
             text_answer = text_answer.replace(",", ".")
-            text_answer = parse_answer(text_answer)
+            parsed_answer = parse_answer(text_answer)
 
-            if text_answer is None:
+            if parsed_answer is None:
                 continue
 
-            text_answers.append(text_answer)
+            text_answers.append(parsed_answer)
 
         generated_description[(a, b, c, d, e, f)] = (agent_contexts, answer)
 
         try:
-            text_answer = most_frequent(text_answers)
-            if text_answer == answer:
+            final_answer = most_frequent(text_answers)
+            if final_answer == answer:
                 scores.append(1)
             else:
                 scores.append(0)
         except:
             continue
 
-        print("performance:", np.mean(scores), np.std(scores) / (len(scores) ** 0.5))
+        performance_mean = np.mean(scores)
+        performance_std = np.std(scores) / (len(scores) ** 0.5)
+        print("performance:", performance_mean, performance_std)
+
+        # Log performance metrics to WandB
+        wandb.log({"performance_mean": performance_mean, "performance_std": performance_std})
 
-    pickle.dump(generated_description, open("math_agents{}_rounds{}.p".format(agents, rounds), "wb"))
-    import pdb
-    pdb.set_trace()
-    print(answer)
-    print(agent_context)
+    # Finish the WandB run
+    wandb.finish()
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 2d17634..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-numpy==1.22.4
-openai==0.27.6
-pandas==1.5.3
-tqdm==4.64.1
