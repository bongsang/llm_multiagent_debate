diff --git a/math/gen_math.py b/math/gen_math.py
index 687f116..c77a738 100644
--- a/math/gen_math.py
+++ b/math/gen_math.py
@@ -1,9 +1,17 @@
-import openai
+from openai import OpenAI
 import json
 import numpy as np
 import time
 import pickle
 from tqdm import tqdm
+import weave  # Import Weave
+import wandb
+
+client = OpenAI()
+OPENAI_MODEL = "gpt-4o-mini"
+
+# Initialize the Weave project
+weave.init('math_agents')
 
 def parse_bullets(sentence):
     bullets_preprocess = sentence.split("\n")
@@ -22,13 +30,13 @@ def parse_bullets(sentence):
 
     return bullets
 
-
+# Weave will track the inputs, outputs, and code of this function
+@weave.op()
 def generate_answer(answer_context):
     try:
-        completion = openai.ChatCompletion.create(
-                  model="gpt-3.5-turbo-0301",
-                  messages=answer_context,
-                  n=1)
+        completion = client.chat.completions.create(
+            model=OPENAI_MODEL, messages=answer_context, n=1
+        )
     except:
         print("retrying due to an error......")
         time.sleep(20)
@@ -36,12 +44,16 @@ def generate_answer(answer_context):
 
     return completion
 
-
+# Weave will track this function as well
+@weave.op()
 def construct_message(agents, question, idx):
 
     # Use introspection in the case in which there are no other agents.
     if len(agents) == 0:
-        return {"role": "user", "content": "Can you verify that your answer is correct. Please reiterate your answer, making sure to state your answer at the end of the response."}
+        return {
+            "role": "user",
+            "content": "Can you verify that your answer is correct? Please reiterate your answer, making sure to state your answer at the end of the response.",
+        }
 
     prefix_string = "These are the recent/updated opinions from other agents: "
 
@@ -51,12 +63,16 @@ def construct_message(agents, question, idx):
 
         prefix_string = prefix_string + response
 
-    prefix_string = prefix_string + "\n\n Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response.".format(question)
+    prefix_string = (
+        prefix_string
+        + "\n\n Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response."
+    )
     return {"role": "user", "content": prefix_string}
 
-
+# Weave will track this function as well
+@weave.op()
 def construct_assistant_message(completion):
-    content = completion["choices"][0]["message"]["content"]
+    content = completion.choices[0].message.content
     return {"role": "assistant", "content": content}
 
 def parse_answer(sentence):
@@ -69,7 +85,6 @@ def parse_answer(sentence):
         except:
             continue
 
-
 def most_frequent(List):
     counter = 0
     num = List[0]
@@ -82,34 +97,59 @@ def most_frequent(List):
 
     return num
 
-
 if __name__ == "__main__":
-    answer = parse_answer("My answer is the same as the other agents and AI language model: the result of 12+28*19+6 is 550.")
+    
+    answer = parse_answer(
+        "My answer is the same as the other agents and AI language model: the result of 12+28*19+6 is 550."
+    )
 
     agents = 2
     rounds = 3
     np.random.seed(0)
 
-    evaluation_round = 100
+    # evaluation_round = 100
+    evaluation_round = 1
     scores = []
 
+    wandb.init(
+        # Set the project where this run will be logged
+        project="llm-multiagent_debate",
+        # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)
+        # Track hyperparameters and run metadata
+        config={
+        "agents": agents,
+        "rounds": rounds,
+        "evaluation_rounds": evaluation_round,
+        })
+
+
     generated_description = {}
 
     for round in tqdm(range(evaluation_round)):
         a, b, c, d, e, f = np.random.randint(0, 30, size=6)
 
         answer = a + b * c + d - e * f
-        agent_contexts = [[{"role": "user", "content": """What is the result of {}+{}*{}+{}-{}*{}? Make sure to state your answer at the end of the response.""".format(a, b, c, d, e, f)}] for agent in range(agents)]
-
-        content = agent_contexts[0][0]['content']
-        question_prompt = "We seek to find the result of {}+{}*{}+{}-{}*{}?".format(a, b, c, d, e, f)
+        agent_contexts = [
+            [
+                {
+                    "role": "user",
+                    "content": f"What is the result of {a}+{b}*{c}+{d}-{e}*{f}? Make sure to state your answer at the end of the response.",
+                }
+            ]
+            for agent in range(agents)
+        ]
+
+        content = agent_contexts[0][0]["content"]
+        question_prompt = f"We seek to find the result of {a}+{b}*{c}+{d}-{e}*{f}?"
 
         for round in range(rounds):
             for i, agent_context in enumerate(agent_contexts):
 
                 if round != 0:
-                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]
-                    message = construct_message(agent_contexts_other, question_prompt, 2*round - 1)
+                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i + 1 :]
+                    message = construct_message(
+                        agent_contexts_other, question_prompt, 2 * round - 1
+                    )
                     agent_context.append(message)
 
                     print("message: ", message)
@@ -123,7 +163,7 @@ if __name__ == "__main__":
         text_answers = []
 
         for agent_context in agent_contexts:
-            text_answer = string =  agent_context[-1]['content']
+            text_answer = agent_context[-1]["content"]
             text_answer = text_answer.replace(",", ".")
             text_answer = parse_answer(text_answer)
 
@@ -145,8 +185,7 @@ if __name__ == "__main__":
 
         print("performance:", np.mean(scores), np.std(scores) / (len(scores) ** 0.5))
 
-    pickle.dump(generated_description, open("math_agents{}_rounds{}.p".format(agents, rounds), "wb"))
-    import pdb
-    pdb.set_trace()
-    print(answer)
-    print(agent_context)
+    # Optionally, you can log the final performance to WandB
+    wandb.log({"performance_mean": np.mean(scores), "performance_std": np.std(scores)})
+
+    wandb.finish()
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 2d17634..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-numpy==1.22.4
-openai==0.27.6
-pandas==1.5.3
-tqdm==4.64.1
