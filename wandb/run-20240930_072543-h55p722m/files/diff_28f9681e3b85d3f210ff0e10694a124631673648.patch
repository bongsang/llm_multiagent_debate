diff --git a/math/gen_math.py b/math/gen_math.py
index 687f116..55099f3 100644
--- a/math/gen_math.py
+++ b/math/gen_math.py
@@ -1,152 +1,147 @@
-import openai
-import json
 import numpy as np
 import time
-import pickle
 from tqdm import tqdm
+import weave
+import wandb
+from openai import OpenAI
 
-def parse_bullets(sentence):
-    bullets_preprocess = sentence.split("\n")
-    bullets = []
-
-    for bullet in bullets_preprocess:
-        try:
-            idx = bullet.find(next(filter(str.isalpha, bullet)))
-        except:
-            continue
-
-        bullet = bullet[idx:]
-
-        if len(bullet) != 0:
-            bullets.append(bullet)
-
-    return bullets
+client = OpenAI()  # Ensure your API key is set in the environment or pass it here
+OPENAI_MODEL = "gpt-4o-mini"
 
+# Initialize Weave project
+weave.init('llm-multiagent-debate-math')
 
+@weave.op()
 def generate_answer(answer_context):
     try:
-        completion = openai.ChatCompletion.create(
-                  model="gpt-3.5-turbo-0301",
-                  messages=answer_context,
-                  n=1)
-    except:
-        print("retrying due to an error......")
+        completion = client.chat.completions.create(
+            model=OPENAI_MODEL, messages=answer_context, n=1
+        )
+    except Exception as e:
+        print(f"Error occurred: {e}\nRetrying after a short delay...")
         time.sleep(20)
         return generate_answer(answer_context)
-
     return completion
 
-
-def construct_message(agents, question, idx):
-
+@weave.op()
+def construct_message(agents, idx):
     # Use introspection in the case in which there are no other agents.
     if len(agents) == 0:
-        return {"role": "user", "content": "Can you verify that your answer is correct. Please reiterate your answer, making sure to state your answer at the end of the response."}
+        return {
+            "role": "user",
+            "content": (
+                "Can you verify that your answer is correct? Please reiterate your "
+                "answer, making sure to state your answer at the end of the response."
+            ),
+        }
 
-    prefix_string = "These are the recent/updated opinions from other agents: "
+    prefix_string = "These are the recent/updated opinions from other agents:"
 
     for agent in agents:
         agent_response = agent[idx]["content"]
-        response = "\n\n One agent response: ```{}```".format(agent_response)
-
-        prefix_string = prefix_string + response
+        response = f"\n\nOne agent response: ```{agent_response}```"
+        prefix_string += response
 
-    prefix_string = prefix_string + "\n\n Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response.".format(question)
+    prefix_string += (
+        "\n\nUse these opinions carefully as additional advice. Can you provide an "
+        "updated answer? Make sure to state your answer at the end of the response."
+    )
     return {"role": "user", "content": prefix_string}
 
-
+@weave.op()
 def construct_assistant_message(completion):
-    content = completion["choices"][0]["message"]["content"]
+    content = completion.choices[0].message.content
     return {"role": "assistant", "content": content}
 
 def parse_answer(sentence):
     parts = sentence.split(" ")
-
-    for part in parts[::-1]:
+    for part in reversed(parts):
         try:
             answer = float(part)
             return answer
-        except:
+        except ValueError:
             continue
+    return None
 
-
-def most_frequent(List):
-    counter = 0
-    num = List[0]
-
-    for i in List:
-        current_frequency = List.count(i)
-        if current_frequency > counter:
-            counter = current_frequency
-            num = i
-
-    return num
-
+def most_frequent(lst):
+    if not lst:
+        return None
+    return max(set(lst), key=lst.count)
 
 if __name__ == "__main__":
-    answer = parse_answer("My answer is the same as the other agents and AI language model: the result of 12+28*19+6 is 550.")
-
     agents = 2
     rounds = 3
     np.random.seed(0)
 
-    evaluation_round = 100
+    evaluation_rounds = 10  # Set this to the desired number of evaluation rounds
     scores = []
 
+    # Initialize WandB
+    wandb.init(
+        project="llm-multiagent-debate-math",
+        config={
+            "agents": agents,
+            "rounds": rounds,
+            "evaluation_rounds": evaluation_rounds,
+        },
+    )
+
     generated_description = {}
 
-    for round in tqdm(range(evaluation_round)):
+    for eval_round in tqdm(range(evaluation_rounds)):
         a, b, c, d, e, f = np.random.randint(0, 30, size=6)
 
-        answer = a + b * c + d - e * f
-        agent_contexts = [[{"role": "user", "content": """What is the result of {}+{}*{}+{}-{}*{}? Make sure to state your answer at the end of the response.""".format(a, b, c, d, e, f)}] for agent in range(agents)]
+        correct_answer = a + b * c + d - e * f
+        initial_question = {
+            "role": "user",
+            "content": (
+                f"What is the result of {a}+{b}*{c}+{d}-{e}*{f}? "
+                "Make sure to state your answer at the end of the response."
+            ),
+        }
 
-        content = agent_contexts[0][0]['content']
-        question_prompt = "We seek to find the result of {}+{}*{}+{}-{}*{}?".format(a, b, c, d, e, f)
+        agent_contexts = [[initial_question] for _ in range(agents)]
 
-        for round in range(rounds):
+        for round_num in range(rounds):
             for i, agent_context in enumerate(agent_contexts):
-
-                if round != 0:
-                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]
-                    message = construct_message(agent_contexts_other, question_prompt, 2*round - 1)
+                if round_num != 0:
+                    # Prepare messages from other agents
+                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i + 1 :]
+                    message = construct_message(agent_contexts_other, 2 * round_num - 1)
                     agent_context.append(message)
 
-                    print("message: ", message)
-
                 completion = generate_answer(agent_context)
-
                 assistant_message = construct_assistant_message(completion)
                 agent_context.append(assistant_message)
-                print(completion)
 
         text_answers = []
 
         for agent_context in agent_contexts:
-            text_answer = string =  agent_context[-1]['content']
-            text_answer = text_answer.replace(",", ".")
-            text_answer = parse_answer(text_answer)
-
-            if text_answer is None:
-                continue
-
-            text_answers.append(text_answer)
-
-        generated_description[(a, b, c, d, e, f)] = (agent_contexts, answer)
-
-        try:
-            text_answer = most_frequent(text_answers)
-            if text_answer == answer:
-                scores.append(1)
-            else:
-                scores.append(0)
-        except:
-            continue
-
-        print("performance:", np.mean(scores), np.std(scores) / (len(scores) ** 0.5))
-
-    pickle.dump(generated_description, open("math_agents{}_rounds{}.p".format(agents, rounds), "wb"))
-    import pdb
-    pdb.set_trace()
-    print(answer)
-    print(agent_context)
+            text_answer = agent_context[-1]["content"].replace(",", ".")
+            parsed_answer = parse_answer(text_answer)
+            if parsed_answer is not None:
+                text_answers.append(parsed_answer)
+
+        generated_description[(a, b, c, d, e, f)] = (agent_contexts, correct_answer)
+
+        final_answer = most_frequent(text_answers)
+        if final_answer == correct_answer:
+            scores.append(1)
+        else:
+            scores.append(0)
+
+        performance_mean = np.mean(scores)
+        performance_std = np.std(scores) / (len(scores) ** 0.5) if len(scores) > 1 else 0
+        print(f"Round {eval_round + 1} Performance:", performance_mean, performance_std)
+
+        # Log performance metrics to WandB
+        wandb.log(
+            {
+                "evaluation_round": eval_round + 1,
+                "performance_mean": performance_mean,
+                "performance_std": performance_std,
+            }
+        )
+
+    # Finish the WandB run
+    wandb.finish()
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 2d17634..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-numpy==1.22.4
-openai==0.27.6
-pandas==1.5.3
-tqdm==4.64.1
